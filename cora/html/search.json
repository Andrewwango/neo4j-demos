[
  {
    "objectID": "cora/demo.html",
    "href": "cora/demo.html",
    "title": "The Graph Advantage",
    "section": "",
    "text": "Import libraries:"
  },
  {
    "objectID": "cora/demo.html#problem-statement",
    "href": "cora/demo.html#problem-statement",
    "title": "The Graph Advantage",
    "section": "1. Problem statement",
    "text": "1. Problem statement\nClassify dataset of academic papers into categories using the Cora dataset.\n\nA research paper citation network\n2708 Paper nodes\n5429 CITES relationships\nEach Paper node has a length 1433 binary feature vector\n\nEach dimension represents a keyword\nValue 1 if paper has keyword, 0 otherwise\n\nEach paper node belongs to a subject\n\n\n\n\n\nnodes = pd.read_csv(\"data/nodes.csv\", converters={\"features\": eval})\nnodes.set_index(\"id\")[[\"subject\", \"features\"]]\n\n\n\n\n\n  \n    \n      \n      subject\n      features\n    \n    \n      id\n      \n      \n    \n  \n  \n    \n      31336\n      Neural_Networks\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      1061127\n      Rule_Learning\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n    \n    \n      1106406\n      Reinforcement_Learning\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      13195\n      Reinforcement_Learning\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      37879\n      Probabilistic_Methods\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1128975\n      Genetic_Algorithms\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      1128977\n      Genetic_Algorithms\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      1128978\n      Genetic_Algorithms\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      117328\n      Case_Based\n      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n    \n      24043\n      Neural_Networks\n      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n    \n  \n\n2708 rows × 2 columns\n\n\n\nPapers also have citations to other papers; citations connect source papers to target papers:\n\nedges = pd.read_csv(\"data/edges.csv\")\nedges[[\"source\", \"target\"]]\n\n\n\n\n\n  \n    \n      \n      source\n      target\n    \n  \n  \n    \n      0\n      1033\n      35\n    \n    \n      1\n      103482\n      35\n    \n    \n      2\n      103515\n      35\n    \n    \n      3\n      1050679\n      35\n    \n    \n      4\n      1103960\n      35\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      5424\n      19621\n      853116\n    \n    \n      5425\n      853155\n      853116\n    \n    \n      5426\n      1140289\n      853118\n    \n    \n      5427\n      853118\n      853155\n    \n    \n      5428\n      1155073\n      954315\n    \n  \n\n5429 rows × 2 columns\n\n\n\nLook at classes:\n\nnodes[\"subject\"].value_counts().plot.bar(title=\"Number of papers per class\");"
  },
  {
    "objectID": "cora/demo.html#tabular-classification",
    "href": "cora/demo.html#tabular-classification",
    "title": "The Graph Advantage",
    "section": "2. Tabular classification",
    "text": "2. Tabular classification\nIn a traditional ML system, we use features of each sample independently of other nodes to classify. We cannot easily model each paper’s citations in a tabular data format.\nTherefore we will only use text features for classifying our papers dataset.\n\nNote: We could construct aggregate numerical features such as how many times each paper cites another paper in a particular category. However this loses information about the spatial structure of the graph. It is hard to randomly split into train-test whilst preventing data leakage.\n\n\nGet features and classes from dataset:\n\nX = nodes[\"features\"].apply(pd.Series).to_numpy()\ny = nodes[\"subject\"].astype(\"category\").cat.codes.to_numpy()\n\nPerform train-test split using scikit-learn and use hyperopt-sklearn to train a Random Forest Classifier and automatically train hyperparameters:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nclassifier = HyperoptEstimator(classifier=random_forest_classifier(\"myclf\"), trial_timeout=10, seed=1)\n\n\nclassifier.fit(X_train, y_train)\n\n100%|████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.18s/trial, best loss: 0.3783783783783784]\n100%|█████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  2.86s/trial, best loss: 0.312039312039312]\n100%|███████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  3.59s/trial, best loss: 0.29484029484029484]\n100%|████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  2.78s/trial, best loss: 0.2702702702702703]\n100%|████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.22s/trial, best loss: 0.2702702702702703]\n100%|████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  3.83s/trial, best loss: 0.2702702702702703]\n100%|████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  2.07s/trial, best loss: 0.2702702702702703]\n100%|████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  1.93s/trial, best loss: 0.2702702702702703]\n100%|████████████████████████████████████████████████████████████████| 9/9 [00:11<00:00, 11.64s/trial, best loss: 0.2702702702702703]\n100%|██████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  6.67s/trial, best loss: 0.2702702702702703]\n\n\nCalculate the classifier accuracy on the test set:\n\nf\"Accuracy: {format(classifier.score(X_test, y_test), '.0%')}\"\n\n'Accuracy: 74%'"
  },
  {
    "objectID": "cora/demo.html#graph-data-classification",
    "href": "cora/demo.html#graph-data-classification",
    "title": "The Graph Advantage",
    "section": "3. Graph data classification",
    "text": "3. Graph data classification\nThis problem is a classic graph data modelling problem. We now can use the citations data since they can be modelled by a graph data format.\nWe can leverage this data model to visualise and extract useful information about the data:\n\nHow do we use the graph for classification? We create spatial features (“embeddings”) for each data point based on their position in the graph. These features can be used for classification.\n\n\n\nWhat are embeddings?\n\nNode embedding algorithms compute low-dimensional vector representations of nodes in a graph. These vectors, also called embeddings, can be used for machine learning.\nThe FastRP algorithm operates on graphs, in which case we care about preserving similarity between nodes and their neighbors. This means that two nodes that have similar neighborhoods should be assigned similar embedding vectors. Conversely, two nodes that are not similar should be not be assigned similar embedding vectors.\n\n\nWe create the embeddings using the FastRP algorithm.\n\nNote: Both FastRP and the downstream classification are statistical techniques and are analogous to simple NLP techniques. We don’t touch on GNNs here (analogous to transformers for text, see here for interesting commentary).\n\n\n3.1 Load dataset into Neo4j\n\nIn Neo4j Desktop, start a local DBMS and install the Graph Data Science Library and APOC plugins.\nMove the data (source) from this repo cora/data/*.csv to the database’s import folder. You can find this under Open/Open folder/Import in Neo4j Desktop.\nLoad the csvs into Neo4j using the following Cypher commands:\n\n\n\nConnect to Neo4j instance\nfrom getpass import getpass\nauth = (\"neo4j\", getpass(\"Password:\"))\nbolt = \"bolt://localhost:7687/neo4j\"\ngds = GraphDataScience(bolt, auth=auth)\n\n\nPassword: ········\n\n\n\n\nLoad data from CSV into Neo4j\ngds.run_cypher(\"\"\"\nLOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\nWITH toInteger(row.id) AS paperId, row.subject AS subject, row.features AS features\nMERGE (p:Paper {paper_Id: paperId})\nSET p.subject = subject, p.features = apoc.convert.fromJsonList(features)\nRETURN count(p)\n\"\"\")\n\ngds.run_cypher(\"\"\"\nLOAD CSV WITH HEADERS FROM \"file:///edges.csv\" AS row\nMATCH(source: Paper {paper_Id: toInteger(row.source)})\nMATCH(target: Paper {paper_Id: toInteger(row.target)})\nMERGE (source)-[r:CITES]->(target)\n\"\"\")\n\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n  \n\n\n\n\nVisualise graph using cypher queries in Neo4j Browser, such as (after colouring nodes with Subject):\nMATCH (p:Paper) -[:CITES]-> (q:Paper) RETURN p,q LIMIT 200\n\n\n\n3.2. Preprocess and project\nUse the gds GraphDataScience client to connect to Neo4j and process the graph. Firstly run simple Cypher query to make categorical label from subject:\n\ngds.run_cypher(\"\"\"\nMATCH (p:Paper)\nSET p.subjectClass = {\n    Neural_Networks: 0, Rule_Learning: 1, Reinforcement_Learning: 2, Probabilistic_Methods: 3, Theory: 4, Genetic_Algorithms: 5, Case_Based: 6\n}[p.subject];\n\"\"\")\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n  \n\n\n\n\nThen project graph to in-memory and return Python object:\n\nG, _ = gds.graph.project(\n   \"cora-graph\",\n   {\"Paper\": {\"properties\": [\"subjectClass\"]} },\n   {\"CITES\": {\"orientation\": \"UNDIRECTED\", \"aggregation\": \"SINGLE\"}}\n);\n\n\n\n\n\n{.python .cell-code code-fold=\"true\" code-summary=\"Check graph memory usage\"\"} G.memory_usage()\n\n'2318 KiB'\n\n\n\n\n3.3. Create node embedding vectors\nWe use embedding vector dimension of 128. We can directly call Neo4j’s fastRP algorithm from the Python client.\n\nresult = gds.fastRP.mutate(\n   G,\n   featureProperties=None,\n   embeddingDimension=128,\n   iterationWeights=[0, 0, 1.0, 1.0],\n   normalizationStrength=0.05,\n   mutateProperty=\"fastRP_Extended_Embedding\"\n)\n\nAfter calling FastRP, stream the results back into Python. Each row of X is a 128-dimensional vector representing one paper.\n\nX = gds.graph.streamNodeProperty(G, 'fastRP_Extended_Embedding')[\"propertyValue\"].apply(pd.Series)\n\n\ny = gds.graph.streamNodeProperty(G, 'subjectClass')[\"propertyValue\"]\n\nTo visualise the embeddings, we use t-SNE algorithm to compress the 128-dimensional vectors into 2D.\n\nfrom sklearn.manifold import TSNE\nX_tsne = TSNE(n_components=2, random_state=0).fit_transform(X)\n\nPlot the 2D embeddings of the papers, coloured by subject:\n\nimport seaborn as sns\ndf_tsne = pd.DataFrame({\"x_1\": X_tsne[:, 0], \"x_2\": X_tsne[:, 1], \"y\": nodes[\"subject\"]})\nsns.scatterplot(df_tsne, x=\"x_1\", y=\"x_2\", hue=\"y\", edgecolor=\"none\").legend(bbox_to_anchor=(1, 1));\n\n\n\n\nWe see that in a severely compressed 2D view, papers are clustered into their classes.\n\n\n3.4 Perform classification with embedding vectors\nNote GDS offers complete ML pipelines for data splitting, training classification models, hyperparameter tuning and inferencing. This is advantageous as it acts directly on embeddings which are stored in the in-memory graph, reducing need to move data. However here we export the embeddings and demonstrate using scikit-learn for familiarity.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nclassifier = HyperoptEstimator(classifier=random_forest_classifier(\"myclf\"), trial_timeout=10, seed=1)\n\n\nclassifier.fit(X_train, y_train)\n\n100%|███████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.93s/trial, best loss: 0.44963144963144963]\n100%|███████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  2.73s/trial, best loss: 0.44963144963144963]\n100%|███████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  2.89s/trial, best loss: 0.44963144963144963]\n100%|███████████████████████████████████████████████████████████████| 4/4 [00:11<00:00, 11.73s/trial, best loss: 0.44963144963144963]\n100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  1.93s/trial, best loss: 0.44963144963144963]\n100%|███████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  3.43s/trial, best loss: 0.16953316953316955]\n100%|███████████████████████████████████████████████████████████████| 7/7 [00:11<00:00, 11.92s/trial, best loss: 0.16953316953316955]\n100%|███████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  3.39s/trial, best loss: 0.16953316953316955]\n100%|███████████████████████████████████████████████████████████████| 9/9 [00:11<00:00, 11.60s/trial, best loss: 0.16953316953316955]\n100%|█████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  4.06s/trial, best loss: 0.16953316953316955]\n\n\nCalculate classifier accuracy on test set:\n\nf\"Accuracy: {format(classifier.score(X_test, y_test), '.0%')}\"\n\n0.8478581979320532"
  },
  {
    "objectID": "cora/demo.html#references",
    "href": "cora/demo.html#references",
    "title": "The Graph Advantage",
    "section": "References",
    "text": "References\nFollowing tutorial. Some images sourced from here. Embedding images source and source."
  }
]